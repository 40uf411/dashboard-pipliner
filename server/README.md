## Alger WebSocket Server

This folder hosts a standalone WebSocket server that speaks the custom **Alger** protocol required by the dashboard project and provides a reproducible reference for pipeline execution, logging, and auditability.

### Layout

```
server/
├── server.py          # WebSocket entrypoint, connection lifecycle, exports
├── app/
│   ├── config.py      # Runtime constants and SQLite path helpers
│   ├── codes.py       # All 2xx/3xx/39x protocol codes
│   ├── context.py     # Dataclasses describing per-connection metadata
│   ├── protocol.py    # Frame parsing/serialization helpers
│   ├── dag/           # DAG execution engine (moved from DAG.py)
│   ├── dag_runner.py  # Glue between the protocol and DAG executor
│   ├── persistence/   # SQLite gateway + schema/migrations
│   └── services.py    # Message handlers, routing, auditing
├── test/              # Integration tests, DAG regression tests, fixtures
├── sqlite_db/         # SQLite artifacts (auto-created; ignored by git)
└── README.md          # This document
```

### Requirements

- Python 3.10+
- [`websockets`](https://websockets.readthedocs.io/) (`pip install websockets`)
- DAG dependencies: `networkx`, `numpy`

### Running the server

```bash
cd server
python server.py
```

The server listens on `ws://localhost:8765` and only accepts connections that:

1. Provide the WebSocket subprotocol `alger`.
2. Include valid credentials via the query string:
   - `username=admin`
   - `password=admin`

Example client URL: `ws://localhost:8765/?username=admin&password=admin`

### What the server tracks

Every request flowing through the WebSocket API ends up in SQLite (`server/sqlite_db/alger.sqlite3`). The gateway in `app/persistence/database.py` records:

- **Users** – credentials, last login, optional metadata.
- **Connections** – IP, origin, headers, subprotocol, connect/disconnect timestamps.
- **Conversations** – logical grouping of all client/server messages for a connection.
- **Messages** – every inbound and outbound Alger frame (IDs, payloads, errors).
- **Executions** – pipeline metadata, status transitions, uploaded parameters.
- **Execution events** – any notable lifecycle milestones (summary saved, failure, stop).
- **Error logs** – structured diagnostics, protocol violations, DAG runtime issues.
- **User actions** – high-level audit trail (login attempt, pipeline list, execution start, stop, output fetch).

Message handlers in `app/services.py` own this instrumentation. If you need more/less telemetry:

- Update `_execution_blocker`, `handle_*` functions, or `_run_and_finalize_execution` to record different events.
- Adjust schema columns or add new tables in `app/persistence/database.py`. The helper `_apply_schema()` can be extended with new migrations.
- Change how summaries are generated by editing `app/dag_runner.py` (e.g., include extra statistics from the DAG result).

### Changing behaviour

- **Credentials / network settings** – edit `app/config.py` (HOST, PORT, USERNAME, PASSWORD, SUBPROTOCOL).
- **SQLite location** – change `SQLITE_DIR` and `DB_PATH` in `app/config.py`.
- **Max concurrency, maintenance mode** – tweak `default_server_state()` or write admin handlers inside `app/services.py`.
- **Execution strategy** – clients can pass `strategy` in message 103/104; add new algorithms in `app/dag/engine.py` and `app/dag_runner.py`.
- **Persistence engine swap** – implement another `PersistenceGateway` and instantiate it inside `app/services.py` instead of the SQLite gateway.
- **Telemetry volume** – `DATABASE.log_message/log_error/record_user_action` calls can be gated/modulated depending on privacy/performance requirements.

### Collaboration guidelines

1. **Branching** – prefer topic branches per feature/fix; avoid pushing directly to main.
2. **Coding style** – stick with the existing module layout (`app/…`, `test/…`). New subsystems should live in their own subpackages.
3. **Tests first** – extend `test/test_server.py` for protocol changes and `test/test_DAG.py` for DAG additions. Run both suites (`python -m unittest …`) before committing.
4. **Schema migrations** – update `_apply_schema()` in `app/persistence/database.py` so existing databases upgrade cleanly. Document new columns under “What the server tracks”.
5. **Docs** – keep this README in sync with API additions, new message types, or behavioural changes. If you introduce new config flags, mention them in “Changing behaviour”.
6. **Logging** – avoid removing log/error statements without providing equivalent observability elsewhere; the telemetry is relied upon for auditing.

Open a PR describing:

- The motivation and scope.
- Schema or protocol changes (include migration/testing steps).
- Screenshots/logs if the change affects observability.
- Verification steps (`python -m unittest …` output).

### Message contract

All frames are JSON objects with the following fields:

| Field      | Type | Description                                                                 |
|------------|------|-----------------------------------------------------------------------------|
| `id`       | int  | Sequential identifier. Client starts at 1; server responses increment by 1.|
| `requestId`| int  | References the `id` that this message responds to (0 if none).             |
| `type`     | int  | Channel semantics (`1xx` requests, `2xx` positive responses, `3xx` errors). |
| `content`  | str  | JSON-encoded string containing the payload.                                |

The server tracks the last observed `id` per connection. Any incoming message whose `id` is not exactly one greater than the last accepted message is ignored.

### Message types

| Request (client) | Success (server) | Error (server) | Description |
|------------------|------------------|----------------|-------------|
| 100 Login        | 200 Login OK     | 300 Unknown user | Authenticates the session payload. |
| 101 Get user data| 201 User data    | 301 Fetch error | Returns static profile information. |
| 102 Get pipeline catalog | 202 Catalog data | 302 Catalog error | Lists available pipeline graphs. |
| 103 Execute pipeline from DB | 203 Execution started | 303 Failed to start/fetch | Launches a stored pipeline using `pipelineId`. |
| 104 Execute ad-hoc pipeline | 204 Execution started | 304 Could not run | Launches a pipeline described in the message `graph`. |
| 106 Stop execution | 206 Stop confirmed | 306 Stop failed | Attempts to stop a running execution via `executionId`. |
| 107 Request execution output | 207 Output payload | 307 Output unavailable | Returns the stored DAG summary (sinks, strategy, stats) for a finished execution. |
| —                | 205 Status update | 305 Status update error | Reserved for server push notifications. |
| —                | 207 Pipeline finished | 307 Pipeline crashed | Server-issued completion or crash notifications. |

### System error codes

- `395` Incorrect client `id`/`requestId`. The server responds with what it expected.
- `396` Unknown or unsupported message type and malformed payloads.
- `397` Too many concurrent executions (capacity reached).
- `398` Pipeline executions halted by an operator.
- `399` Server in maintenance mode.

Extend the handlers inside `server.py` if you need richer domain-specific logic or additional message types.

### Testing

Integration suite (runs the WebSocket server in-process):

```bash
cd server
python -m unittest test.test_server
```

DAG regression suite (validates the execution engine independently):

```bash
python -m unittest test.test_DAG
```

Both suites reset the SQLite database between tests, seed the default pipeline from `test/test_DAG.json`, and assert that execution summaries are stored and retrievable.
